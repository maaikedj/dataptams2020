{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Challenge 1 - Import and Describe the Dataset\n",
    "\n",
    "In this lab, we will use a dataset containing information about customer preferences. We will look at how much each customer spends in a year on each subcategory in the grocery store and try to find similarities using clustering.\n",
    "\n",
    "The dataset is located [here](https://drive.google.com/file/d/1z1gYSD32ktbHuKSzB5JVS_u4YsLibh5F/view?usp=sharing), please download it and place it in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# loading the data:\n",
    "\n",
    "customers = pd.read_csv('../data/Wholesale_customer_ data.csv')\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Explore the dataset with mathematical and visualization techniques. What do you find?\n",
    "\n",
    "Checklist:\n",
    "\n",
    "* What does each column mean?\n",
    "* Any categorical data to convert?\n",
    "* Any missing data to remove?\n",
    "* Column collinearity - any high correlations?\n",
    "* Descriptive statistics - any outliers to remove?\n",
    "* Column-wise data distribution - is the distribution skewed?\n",
    "* Etc.\n",
    "\n",
    "Additional info: Over a century ago, an Italian economist named Vilfredo Pareto discovered that roughly 20% of the customers account for 80% of the typical retail sales. This is called the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle). Check if this dataset displays this characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "customers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "corr = customers.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr, xticklabels = corr.columns, yticklabels = corr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.boxplot(figsize = (12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 3, figsize=(16, 8), sharex=True)\n",
    "sns.distplot(customers['Fresh'], ax=axes[0, 0])\n",
    "sns.distplot(customers['Milk'], ax=axes[0, 1])\n",
    "sns.distplot(customers['Grocery'], ax=axes[0, 2])\n",
    "sns.distplot(customers['Frozen'], ax=axes[1, 0])\n",
    "sns.distplot(customers['Detergents_Paper'], ax=axes[1, 1])\n",
    "sns.distplot(customers['Delicassen'], ax=axes[1, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your observations here\n",
    "\n",
    "# The first two columns seem to be categorical columns ('Channel' only has values 1 and 2, 'Region' only 1,2 and 3)\n",
    "# The other columns are types of groceries and should all be numerical columns, which they are\n",
    "# There is no missing data\n",
    "# Only two columns are highly correlated: 'Grocery' and 'Detergents_Paper' (r = 0,92), so one should be removed\n",
    "# The distribution is positively skewed for all numerical variables, so possibly a square root, cube root, or log transformation could help here\n",
    "# There are many outliers to the right, but this is probably the result of the skewness and they are probably valid data points. \n",
    "# I don't see reason to remove them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Challenge 2 - Data Cleaning and Transformation\n",
    "\n",
    "If your conclusion from the previous challenge is the data need cleaning/transformation, do it in the cells below. However, if your conclusion is the data need not be cleaned or transformed, feel free to skip this challenge. But if you do choose the latter, please provide rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# First, remove 'Detergents_Paper' columns as it's highly correlated with the 'Grocery' column\n",
    "\n",
    "customers.drop(columns = 'Detergents_Paper', axis = 1, inplace = True)\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# I'm going to approach data conversion in several ways as I see different possible options.\n",
    "# I'll compare the models with the different conversion approaches to see which one works best\n",
    "\n",
    "# The first two columns ('Channel' and 'Region') have numerical type but behave as categorical data (and according to data source are nominal data)\n",
    "# So, I will test three versions: one with the columns as numerical data (as they are), one without these columns, and one with the one-hot encoded columns\n",
    "\n",
    "# customers = version with all numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customers1 will be version with one-hot encoding for first two columsn\n",
    "# convert data type from numerical to categorical for first two colums\n",
    "\n",
    "customers1 = customers.copy()\n",
    "customers1['Channel'] = customers1['Channel'].apply(str)\n",
    "customers1['Region'] = customers1['Region'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode first two columns:\n",
    "\n",
    "customers1_dummy = pd.get_dummies(customers1, columns = ['Channel', 'Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers1_dummy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers1_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customers2 without the first two columns\n",
    "\n",
    "customers2 = customers.drop(columns = ['Channel', 'Region'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the customers2 version, I'm also going to try the effect of transforming the numerical variables so they're less skewed\n",
    "# I'm trying three different transformations: log, square root and cube root\n",
    "\n",
    "# Apply log conversion to remaining numerical columns\n",
    "\n",
    "customers2_log = customers2.apply(lambda x: np.log10(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check whether distribution improved\n",
    "\n",
    "f, axes = plt.subplots(2, 3, figsize=(16, 8), sharex=True)\n",
    "sns.distplot(customers2_log['Fresh'], ax=axes[0, 0])\n",
    "sns.distplot(customers2_log['Milk'], ax=axes[0, 1])\n",
    "sns.distplot(customers2_log['Grocery'], ax=axes[0, 2])\n",
    "sns.distplot(customers2_log['Frozen'], ax=axes[1, 0])\n",
    "sns.distplot(customers2_log['Delicassen'], ax=axes[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply square root conversion to numerical columns\n",
    "\n",
    "customers2_sqrt = customers2.apply(lambda x: np.sqrt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check whether distribution improved\n",
    "\n",
    "f, axes = plt.subplots(2, 3, figsize=(16, 8), sharex=True)\n",
    "sns.distplot(customers2_sqrt['Fresh'], ax=axes[0, 0])\n",
    "sns.distplot(customers2_sqrt['Milk'], ax=axes[0, 1])\n",
    "sns.distplot(customers2_sqrt['Grocery'], ax=axes[0, 2])\n",
    "sns.distplot(customers2_sqrt['Frozen'], ax=axes[1, 0])\n",
    "sns.distplot(customers2_sqrt['Delicassen'], ax=axes[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply cube root conversion to numerical columns\n",
    "\n",
    "customers2_cubrt = customers2.apply(lambda x: np.cbrt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually check whether distribution improved\n",
    "\n",
    "f, axes = plt.subplots(2, 3, figsize=(16, 8), sharex=True)\n",
    "sns.distplot(customers2_cubrt['Fresh'], ax=axes[0, 0])\n",
    "sns.distplot(customers2_cubrt['Milk'], ax=axes[0, 1])\n",
    "sns.distplot(customers2_cubrt['Grocery'], ax=axes[0, 2])\n",
    "sns.distplot(customers2_cubrt['Frozen'], ax=axes[1, 0])\n",
    "sns.distplot(customers2_cubrt['Delicassen'], ax=axes[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# The cuberoot transformation seems to best improve the skew of the data so I'll go forward with that one\n",
    "\n",
    "# check outliers in the transformed data:\n",
    "\n",
    "customers2_cubrt.boxplot(figsize = (12, 8))\n",
    "\n",
    "# there are considerably fewer outliers than with the untransformed data\n",
    "# not sure whether to remove them - as they seem to be valid data points and not errors I choose not to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now I have 4 versions of the data:\n",
    "# customers: first two columns as numerical, data untransformed\n",
    "# customers1_dummy: first two columns categorical one hot encoded\n",
    "# customers2: first two columns dropped\n",
    "# customers2_cubrt: variables transformed with cube root transformation to improve skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Challenge 3 - Data Preprocessing\n",
    "\n",
    "One problem with the dataset is the value ranges are remarkably different across various categories (e.g. `Fresh` and `Grocery` compared to `Detergents_Paper` and `Delicassen`). If you made this observation in the first challenge, you've done a great job! This means you not only completed the bonus questions in the previous Supervised Learning lab but also researched deep into [*feature scaling*](https://en.wikipedia.org/wiki/Feature_scaling). Keep on the good work!\n",
    "\n",
    "Diverse value ranges in different features could cause issues in our clustering. The way to reduce the problem is through feature scaling. We'll use this technique again with this dataset.\n",
    "\n",
    "#### We will use the `StandardScaler` from `sklearn.preprocessing` and scale our data. Read more about `StandardScaler` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
    "\n",
    "*After scaling your data, assign the transformed data to a new variable `customers_scale`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your import here:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Your code here:\n",
    "\n",
    "scaler = StandardScaler()\n",
    "customers_scale = scaler.fit_transform(customers)\n",
    "customers_scale = pd.DataFrame(customers_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_scale1 = scaler.fit_transform(customers1_dummy)\n",
    "customers_scale1 = pd.DataFrame(customers_scale1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_scale2 = scaler.fit_transform(customers2)\n",
    "customers_scale2 = pd.DataFrame(customers_scale2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_scale2_cubrt = scaler.fit_transform(customers2_cubrt)\n",
    "customers_scale2_cubrt = pd.DataFrame(customers_scale2_cubrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Challenge 3 - Data Clustering with K-Means\n",
    "\n",
    "Now let's cluster the data with K-Means first. Initiate the K-Means model, then fit your scaled data. In the data returned from the `.fit` method, there is an attribute called `labels_` which is the cluster number assigned to each data record. What you can do is to assign these labels back to `customers` in a new column called `customers['labels']`. Then you'll see the cluster results of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "customer_kmeans = KMeans().fit(customers_scale)\n",
    "customers['labels'] = customer_kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Count the values in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "customers['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Challenge 4 - Data Clustering with DBSCAN\n",
    "\n",
    "Now let's cluster the data using DBSCAN. Use `DBSCAN(eps=0.5)` to initiate the model, then fit your scaled data. In the data returned from the `.fit` method, assign the `labels_` back to `customers['labels_DBSCAN']`. Now your original data have two labels, one from K-Means and the other from DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "customers_dbscan = DBSCAN(eps=0.5).fit(customers_scale)\n",
    "\n",
    "customers['labels_DBSCAN'] = customers_dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Count the values in `labels_DBSCAN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "customers['labels_DBSCAN'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Challenge 5 - Compare K-Means with DBSCAN\n",
    "\n",
    "Now we want to visually compare how K-Means and DBSCAN have clustered our data. We will create scatter plots for several columns. For each of the following column pairs, plot a scatter plot using `labels` and another using `labels_DBSCAN`. Put them side by side to compare. Which clustering algorithm makes better sense?\n",
    "\n",
    "Columns to visualize:\n",
    "\n",
    "* `Detergents_Paper` as X and `Milk` as y\n",
    "* `Grocery` as X and `Fresh` as y\n",
    "* `Frozen` as X and `Delicassen` as y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Visualize `Detergents_Paper` as X and `Milk` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "# I removed 'Detergents_Paper' from the data as it was higly correlated with 'Grocery'\n",
    "# I'll replace it here with 'Grocery'\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize = (16,6))\n",
    "\n",
    "ax1.scatter(x=customers['Grocery'], y=customers['Milk'], c=customers['labels'])\n",
    "ax1.set_title('K-Means')\n",
    "ax1.set_xlabel('Grocery')\n",
    "ax1.set_ylabel('Milk')\n",
    "\n",
    "ax2.scatter(x=customers['Grocery'], y=customers['Milk'], c=customers['labels_DBSCAN'])\n",
    "ax2.set_title('DBSCAN')\n",
    "ax2.set_xlabel('Grocery')\n",
    "ax2.set_ylabel('Milk')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Visualize `Grocery` as X and `Fresh` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize = (16,6))\n",
    "\n",
    "ax1.scatter(x=customers['Grocery'], y=customers['Fresh'], c=customers['labels'])\n",
    "ax1.set_title('K-Means')\n",
    "ax1.set_xlabel('Grocery')\n",
    "ax1.set_ylabel('Fresh')\n",
    "\n",
    "ax2.scatter(x=customers['Grocery'], y=customers['Fresh'], c=customers['labels_DBSCAN'])\n",
    "ax2.set_title('DBSCAN')\n",
    "ax2.set_xlabel('Grocery')\n",
    "ax2.set_ylabel('Fresh')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Visualize `Frozen` as X and `Delicassen` as y by `labels` and `labels_DBSCAN` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1,2, figsize = (16,6))\n",
    "\n",
    "ax1.scatter(x=customers['Frozen'], y=customers['Delicassen'], c=customers['labels'])\n",
    "ax1.set_title('K-Means')\n",
    "ax1.set_xlabel('Frozen')\n",
    "ax1.set_ylabel('Delicassen')\n",
    "\n",
    "ax2.scatter(x=customers['Frozen'], y=customers['Delicassen'], c=customers['labels_DBSCAN'])\n",
    "ax2.set_title('DBSCAN')\n",
    "ax2.set_xlabel('Frozen')\n",
    "ax2.set_ylabel('Delicassen')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's use a groupby to see how the mean differs between the groups. Group `customers` by `labels` and `labels_DBSCAN` respectively and compute the means for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "customers.groupby('labels').agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.groupby('labels_DBSCAN').agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Which algorithm appears to perform better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your observations here\n",
    "\n",
    "# Perhaps I'm doing something wrong, but I really can't say which one performs better based on these figures or numbers, it's really unclear to me\n",
    "\n",
    "# So I'm going to leave the comparison with the three other formatted datasets, because I can't really make sense of the first case\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Bonus Challenge 2 - Changing K-Means Number of Clusters\n",
    "\n",
    "As we mentioned earlier, we don't need to worry about the number of clusters with DBSCAN because it automatically decides that based on the parameters we send to it. But with K-Means, we have to supply the `n_clusters` param (if you don't supply `n_clusters`, the algorithm will use `8` by default). You need to know that the optimal number of clusters differs case by case based on the dataset. K-Means can perform badly if the wrong number of clusters is used.\n",
    "\n",
    "In advanced machine learning, data scientists try different numbers of clusters and evaluate the results with statistical measures (read [here](https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation)). We are not using statistical measures today but we'll use our eyes instead. In the cells below, experiment with different number of clusters and visualize with scatter plots. What number of clusters seems to work best for K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your comment here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Bonus Challenge 3 - Changing DBSCAN `eps` and `min_samples`\n",
    "\n",
    "Experiment changing the `eps` and `min_samples` params for DBSCAN. See how the results differ with scatter plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your comment here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
