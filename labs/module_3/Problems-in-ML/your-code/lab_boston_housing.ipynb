{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Understanding Over & Underfitting\n",
    "## Predicting Boston Housing Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Getting Started\n",
    "In this project, you will use the Boston Housing Prices dataset to build several models to predict the prices of homes with particular qualities from the suburbs of Boston, MA.\n",
    "We will build models with several different parameters, which will change the goodness of fit for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "---\n",
    "## Data Exploration\n",
    "Since we want to predict the value of houses, the **target variable**, `'MEDV'`, will be the variable we seek to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Import and explore the data. Clean the data for outliers and missing values. \n",
    "Download the data from [here](https://drive.google.com/file/d/1o-vZHHSywBksnPuGRunvpdYN7grYbe8h/view?usp=sharing) and place it in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "boston = pd.read_csv('../data/boston_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.dtypes\n",
    "\n",
    "# all data types are numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.isnull().sum()\n",
    "\n",
    "# no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16,10))\n",
    "\n",
    "#sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(boston))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "# Normally I wouldn't just remove all outliers like this without inspection and good reason\n",
    "# but because this is just practise and I don't know what the variables mean I'm removing all rows with at least one outlier\n",
    "\n",
    "boston2 = boston[(np.abs(stats.zscore(boston)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.shape)\n",
    "print(boston2.shape)\n",
    "\n",
    "# 83 rows were removed because they contained at least one outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Next, we want to explore the data. Pick several varibables you think will be ost correlated with the prices of homes in Boston, and create plots that show the data dispersion as well as the regression line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your plots here\n",
    "\n",
    "boston2.corr()\n",
    "\n",
    "# from this table it appears that two variables are most correlated with house price (medv): 'rm' (r = 0.71) and 'lstat' (r = -0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll plot those two variables against medv\n",
    "\n",
    "sns.lmplot(x ='lstat', y ='medv', data = boston2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x ='rm', y ='medv', data = boston2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### What do these plots tell you about the relationships between these variables and the prices of homes in Boston? Are these the relationships you expected to see in these variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your response here\n",
    "\n",
    "# lstat seems to be negatively correlated with house price\n",
    "# rm seems to be posititively correlated with house price\n",
    "# I don't know what lstat or rm means so I can't say what expected to see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Make a heatmap of the remaining variables. Are there any variables that you did not consider that have very high correlations? What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your response here\n",
    "corr = boston2.drop(columns = ['lstat', 'rm'], axis = 1).corr()\n",
    "sns.heatmap(corr)\n",
    "\n",
    "# there are no other high correlations\n",
    "# I do see that something strange is going on with variable 'chas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston2['chas'].value_counts()\n",
    "\n",
    "# boston2 has value 0.0 for all instances so that column should be removed for the rest of the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Calculate Statistics\n",
    "Calculate descriptive statistics for housing price. Include the minimum, maximum, mean, median, and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "boston['medv'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "----\n",
    "\n",
    "## Developing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Implementation: Define a Performance Metric\n",
    "What is the performance meteric with which you will determine the performance of your model? Create a function that calculates this performance metric, and then returns the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    # Your code here:\n",
    "    return r2_score(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Implementation: Shuffle and Split Data\n",
    "Split the data into the testing and training datasets. Shuffle the data as well to remove any bias in selecting the traing and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "y = boston2['medv']\n",
    "X = boston2.drop('medv', axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# according to the documentation train_test_split shuffles the data by default "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "----\n",
    "\n",
    "## Analyzing Model Performance\n",
    "Next, we are going to build a Random Forest Regressor, and test its performance with several different parameter settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Learning Curves\n",
    "Lets build the different models. Set the max_depth parameter to 2, 4, 6, 8, and 10 respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Five separate RFR here with the given max depths\n",
    "\n",
    "RFR2 = RandomForestRegressor(max_depth=2)\n",
    "RFR4 = RandomForestRegressor(max_depth=4)\n",
    "RFR6 = RandomForestRegressor(max_depth=6)\n",
    "RFR8 = RandomForestRegressor(max_depth=8)\n",
    "RFR10 = RandomForestRegressor(max_depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now, plot the score for each tree on the training set and on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Produce a plot with the score for the testing and training for the different max depths\n",
    "\n",
    "models = [RFR2, RFR4, RFR6, RFR8, RFR10]\n",
    "r2score_train = []\n",
    "r2score_test = []\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_train)\n",
    "    r2score_train.append(r2_score(y_train, y_pred))\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    r2score_test.append(r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r2 = pd.DataFrame(list(zip(r2score_train, r2score_test)), index = ['RFR2', 'RFR4', 'RFR6', 'RFR8', 'RFR10'], columns =['r2_train', 'r2_test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('r2_train', 'r2_test', data = df_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "What do these results tell you about the effect of the depth of the trees on the performance of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your response here\n",
    "\n",
    "# Looking at the figure and the r2 scores in the table df_r2, the higher the max depth, the better the model performs\n",
    "# The model with max depth = 10 has the highest r2 score for the train and the test set\n",
    "# but the r2 scores for the models with max depths of 6,8 and 10 are very similar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Bias-Variance Tradeoff\n",
    "When the model is trained with a maximum depth of 1, does the model suffer from high bias or from high variance? How about when the model is trained with a maximum depth of 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your response here\n",
    "\n",
    "# When the model is trained with a maximum depth of 1 there is a high bias, \n",
    "# when the depth increases the bias becomes lower but the variance higher\n",
    "# so max depth 10 has low bias but high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Best-Guess Optimal Model\n",
    "What is the max_depth parameter that you think would optimize the model? Run your model and explain its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your response here\n",
    "\n",
    "# I think max depth of 6 as a higher max depth doesn't improve the model score much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Applicability\n",
    "*In a few sentences, discuss whether the constructed model should or should not be used in a real-world setting.*  \n",
    "**Hint:** Some questions to answering:\n",
    "- *How relevant today is data that was collected from 1978?*\n",
    "- *Are the features present in the data sufficient to describe a home?*\n",
    "- *Is the model robust enough to make consistent predictions?*\n",
    "- *Would data collected in an urban city like Boston be applicable in a rural city?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Your response here\n",
    "\n",
    "# I guess the data isn't very relevant today anymore as the housing situation has changed a lot since 1978\n",
    "# I don't know what the columns/ features mean (did I miss something?)\n",
    "# The model does seem to score quite well (or am I wrong?)\n",
    "# the data collected in Boston would probably not be representative for a rural city"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
