{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Storytelling Data Visualization Lab\n",
    "\n",
    "In this lab you'll use a dataset called `housing_prices.csv` which contains the sales data of houses. The dataset and descriptions of the columns are available from [Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). For your convenience, you can review the descriptions of the data columns from [here](https://drive.google.com/file/d/1-iXooLjNuEXU41dqz8ORQ5JEZPHd9x0X/view?usp=sharing).\n",
    "\n",
    "Pretend you are a data analyst at an investment company where the board decided to make investments in real estates. Your boss asked you to analyze this housing sales dataset and present to the investment managers on **what features of houses are strong indicators of the final sale price**. You need to present your findings in intuitive ways so that the investment managers understand where your conclusions come from.\n",
    "\n",
    "#### You will use the appropriate data visualization graphs to tell your stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Challenge 1 - Understanding the Dataset\n",
    "\n",
    "After receiving the data and clarifying your objectives with your boss, you will first try to understand the dataset. This allows you to decide how you will start your research in the next step.\n",
    "\n",
    "The dataset is [here](https://drive.google.com/file/d/1MRhRtdX8QuPPEhelBIS_FEl5vJjRLSeE/view?usp=sharing). Please download it and place it in the data folder.<br>\n",
    "First, import the basic libraries and the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('../data/housing_prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### As a routine before analyzing a dataset, print the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "You find the dataset has 81 columns which are a lot. \n",
    "\n",
    "#### Since the column `Id` is meaningless in our data visualization work, let's drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "df.drop('Id',1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "You care about missing values. If a column has too many missing values, it is not reliable to use it to predict sales price.\n",
    "\n",
    "#### In the cell below, calculate the percentage of missing values for each column. \n",
    "\n",
    "Make a table containing the column name and the percentage of missing values. Print the columns where more than 20% of values are missing. An example of what your output  should look like is [here](https://drive.google.com/file/d/1cuq6qhFZC5wavm-_STcxktBKdAc4xvH8/view?usp=sharing)\n",
    "\n",
    "[This reference](https://stackoverflow.com/questions/51070985/find-out-the-percentage-of-missing-values-in-each-column-in-the-given-dataset) can help you make the missing values table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Drop the columns you find that have more than 20% missing values.\n",
    "\n",
    "After dropping, check the shape of your dataframes. You should have 75 columns now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "for column in missing_value_df['column_name'][missing_value_df.percent_missing > 20]:\n",
    "    df.drop(column,1,inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Since you're asked to analyze sale prices, first let's see if the sale prices (column `SalePrice`) has a normal distribution. This is important because normally distributed data can be better represented with mathematical models.\n",
    "\n",
    "#### In the cell below, use the propriate graph to visualize the shape of distribution of the sale prices. Then explain what you find from the graph about data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "sns.distplot(df['SalePrice'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# your comment here\n",
    "#the column does seem to have a normal distribution\n",
    "#just a little bit skewed to the left\n",
    "#so most items sold are around the mean price, not too much standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Bonus Challenge 1 - Adjust Data Distribution\n",
    "\n",
    "If you used the correct method in the previous step, you should have found the data distribution is skewed to the left. In order to improve your data visualization in the next steps, you can opt to adjust the `SalePrice` column by applying a mathematical function to the values. The goal is to produce a bell-shape normal distribution after applying the mathematical function to the sale price.\n",
    "\n",
    "*This technique is optional in data visualization but you'll find it useful in your future machine learning analysis.*\n",
    "\n",
    "#### In the cell below, adjust the `SalePrice` column so that the data are normally distributed.\n",
    "\n",
    "Try applying various mathematical functions such as square root, power, and log to the `SalePrice` column. Visualize the distribution of the adjusted data until you find a function that makes the data normally distributed. **Create a new column called `SalePriceAdjusted` to store the adjusted sale price.**\n",
    "\n",
    "[This reference](https://trainingdatascience.com/workshops/histograms-and-skewed-data/) shows you examples on how to adjust skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "#power of 2 : becomes much more skewed to the left\n",
    "#df['SalePriceAdjusted'] = df['SalePrice']**2\n",
    "\n",
    "\n",
    "#square root : looks much better distributed\n",
    "#df['SalePriceAdjusted'] = df['SalePrice']**(1/2)\n",
    "\n",
    "#log: \n",
    "df['SalePriceAdjusted'] = np.log(df['SalePrice'])\n",
    "\n",
    "sns.distplot(df['SalePriceAdjusted'])\n",
    "\n",
    "#log returns a much nicer distribution of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Challenge 2 - Exploring Data with Common Sense\n",
    "\n",
    "Now that we have a general understanding of the dataset, we start exploring the data with common sense by means of data visualization. Yes, in data analysis and even machine learning you are often required to use common sense. You use your common sense to make a scientific guess (i.e. hypothesis) then use data analytics methods to test your hypothesis.\n",
    "\n",
    "This dataset is about housing sales. According to common sense, housing prices depend on the following factors:\n",
    "\n",
    "* **Size of the house** (`GrLivArea`, `LotArea`, and `GarageArea`).\n",
    "\n",
    "* **Number of rooms** (`BedroomAbvGr`, `KitchenAbvGr`, `FullBath`, `HalfBath`, `BsmtFullBath`, `BsmtHalfBath`).\n",
    "\n",
    "* **How long the house has been built or remodeled** (`YearBuilt` and `YearRemodAdd`).\n",
    "\n",
    "* **Neighborhood of the house** (`Neighborhood`).\n",
    "\n",
    "#### In this challenge, use the appropriate graph type to visualize the relationships between `SalePrice` (or `SalePriceAdjusted`) and the fields above. \n",
    "\n",
    "Note that:\n",
    "\n",
    "* Transform certain columns in order to visualize the data properly based on common sense. For example:\n",
    "    * Visualizing how the number of half bathrooms affected the sale price probably does not make sense. You can create a new column to calculate the total number of bathrooms/rooms then visualize with the calculated number.\n",
    "    * `YearBuilt` and `YearRemodAdd` are year numbers not the age of the house. You can create two new columns for how long the house has been built or remodeled then visualize with the calculated columns.\n",
    "* Make comments to explain your thinking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# add cells as needed\n",
    "#new columns:\n",
    "df['size_house'] = df['GrLivArea'] + df['LotArea'] + df['GarageArea']\n",
    "\n",
    "df['number_rooms'] = df['BedroomAbvGr'] + df['KitchenAbvGr'] + df['FullBath'] + df['HalfBath'] + df['BsmtFullBath'] + df['BsmtHalfBath']\n",
    "\n",
    "df['age'] = 2020 - df['YearBuilt']\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping columns I want:\n",
    "df_new = df[['size_house','number_rooms','age','SalePriceAdjusted']]\n",
    "sns.pairplot(df_new, diag_kind= 'hist')\n",
    "#if we look at the last column we see the price of the house vs the other columns in scatter plots\n",
    "# there doesn't seem to be a specific trend for any of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping columns I want:\n",
    "df_new = df[['size_house','number_rooms','age','SalePrice']]\n",
    "sns.pairplot(df_new, diag_kind= 'hist')\n",
    "#wanted to see the difference without the adjusted price\n",
    "#also hard to see a trend in any of the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Bonus Challenge 2 - Exploring Data with Correlation Heatmap\n",
    "\n",
    "Now you have explored data visualizations with certain fields based on common sense. In the dataset there are many other fields that you are not sure whether they are important factors for the sale price. What is the best way to explore those fields without investigating them individually?\n",
    "\n",
    "Making scatter matrix is not an option here because there are too many fields which makes it extremely time consuming to create scatter matrix. One option you have is to create a heatmap. Heatmaps are much less expensive to create than scatter matrixes. You can use heatmaps to visualize the pairwise correlations between each two variables.\n",
    "\n",
    "Here is a [reference](https://seaborn.pydata.org/examples/many_pairwise_correlations.html) you can use to learn how to creat the pairwise correlation heatmap. Your heatmap should look like this [example](https://drive.google.com/file/d/1JhdNvbAnnWDFXEtDoBtx3B2KKIkqsnSH/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "corr = df.corr()\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "# not my code, source: https://seaborn.pydata.org/examples/many_pairwise_correlations.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In your heatmap, you can easily identify the highly correlated (either positively or negatively) variables by looking for the grids with darker colors. \n",
    "\n",
    "#### In the cell below, summarize what variables are highly correlated to the sale price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# your comment here\n",
    "# the price doesn't seem to be negative correlated to any other variable\n",
    "# it seems however highly correlated to many variables to do with AREA:\n",
    "# LotArea\n",
    "# OverallQual: Overall material and finish quality\n",
    "# TotalBsmtSF: Total square feet of basement area\n",
    "# 1stFlrSF: First Floor square feet\n",
    "# 2ndFlrSF: Second floor square feet\n",
    "# GrLivArea: Above grade (ground) living area square feet\n",
    "# Fireplaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Challenge 3 - Present Your Stories\n",
    "\n",
    "Now based on your findings from the explorations, summarize and present your stories.\n",
    "\n",
    "#### Present the top 5 factors that affect the sale price.\n",
    "\n",
    "Use the following format to present each factor:\n",
    "\n",
    "1. A title line about the factor.\n",
    "\n",
    "1. No more than 3 sentences to describe the relationship between the factor and the sale price.\n",
    "\n",
    "1. Support your point with the appropriate graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# your responses here\n",
    "# add cells as needed\n",
    "\n",
    "#it seems that the most important factor are Area and number of rooms\n",
    "#which seems very obvious as bigger house = more money\n",
    "#its nice to see it confirmed anyway\n",
    "#its however interesting that the age of the house is not so much a factor\n",
    "#I guess we could deduce that older houses are also expensive as they have charm and could be classical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
