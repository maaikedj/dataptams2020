{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "DPmPjkf8GRT5"
   },
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ab6TB9NMGPfs"
   },
   "source": [
    "## Introduction \n",
    "\n",
    "Web scraping refers to the automatic extraction of information from a web page. This information is often a page's content, but it can also include information in the page's headers, links present on the page, or any other information embedded in the page's HTML. Because of this, scraping has become one of the most popular ways to extract data from the web. With basic knowledge of HTML and the help of a few Python libraries, you can obtain information from just about any page on the internet.\n",
    "\n",
    "In this lesson, we will cover the basics of web scraping with Python and show examples of how to scrape text content from a simple web page as well the more complex task of extracting data from an HTML table embedded on a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "f3vrX3VFGXp-"
   },
   "source": [
    "## Scraping a Simple Web Page\n",
    "\n",
    "Scraping a simple website is relatively straightforward. The first thing we need to do is determine the web page we want to scrape and the information we would like to obtain from it. For our purposes, let's suppose we wanted to scrape a Reuters news article and we wanted to extract the main text content (article title, story, etc.).\n",
    "\n",
    "We first need to specify the URL of the page we want to scrape and then use the requests library's get method to request the page and the content method to retrieve the HTML content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "h9BfurHKGVBk"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.reuters.com/article/us-shazam-m-a-apple-eu/eu-clears-apples-purchase-of-shazam-idUSKCN1LM1TZ'\n",
    "html = requests.get(url)\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get the content\n",
    "html = requests.get(url).content\n",
    "#html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "GHuVmSH2Gg6X"
   },
   "source": [
    "While printing the first 600 characters of the HTML content, you'll see something like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "RZytdqS1GtNQ"
   },
   "outputs": [],
   "source": [
    "# Show the first 600 chars\n",
    "html[0:600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "BWLTBgEqG13Y"
   },
   "source": [
    "\n",
    "As you can see, there is a lot of extra information here that we don't really need if all we are interested in is the text content from the page. We will need to perform a few steps to clean this up, the first of which is to use the BeautifulSoup library to read the raw HTML and structure it in a way where we will be able to more easily parse the information we want out of it. In BeautifulSoup terms, this is called \"making the soup.\"\n",
    "\n",
    "In order to run this code, you will need to install a lxml-parser. If you haven't done so already, please pip install it using the following link:\n",
    "\n",
    "https://pypi.org/project/lxml/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "xsgjNkJgG4XW"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "https://www.reuters.com/article/us-shazam-m-a-apple-eu/eu-clears-apples-purchase-of-shazam-idUSKCN1LM1TZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='webscrap_exmp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ZB07t4OqG7Sx"
   },
   "source": [
    "You can see that our soup is slightly more structured than our raw HTML, but the best part about BeautifulSoup comes next. It allows us to extract specific HTML elements from the soup we have created using the find_all method. In our case, we are going to use it to find and extract all the text contained within header tags and paragraph tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "i3hUN1zmGhTv"
   },
   "outputs": [],
   "source": [
    "tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p']\n",
    "text = [element.text for element in soup.find_all(tags)]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "soup.find_all('h1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "M16jy2tfG927"
   },
   "source": [
    "This gives us a neat list where the text of each HTML element BeautifulSoup found is an element in the list. If we want to view it in paragraph form, we can simply call the join method, use a new line (\\n) to join the elements together, and we get the text neatly in paragraph form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "zM6zKZJ0HBrS"
   },
   "outputs": [],
   "source": [
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "zQzOPh9FHEhh"
   },
   "source": [
    "## More Complex Simple Page Scraping\n",
    "\n",
    "The previous example was relatively straightforward because we were just extracting the text content from the page. Suppose we wanted to extract data that was contained within an HTML table and store it in a Pandas data frame. This objective makes our scraping task a bit more complex as we would need to identify the table within the HTML, identify the rows within the table, and then read and format the information within those rows so that they fit within a data frame. Let's look at an example of how we would extract a table containing life expectancies for each European country from Wikipedia.\n",
    "\n",
    "This task would start out just like the previous one. We would specify the URL, use the requests library to request the page and retrieve the raw HTML content, and turn the HTML into soup using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "S-MEuQ2rHHe4",
    "outputId": "42d5e9b3-aecb-4bd2-9538-d95b99a5b0a1"
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_European_countries_by_life_expectancy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#html = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "html = requests.get('https://en.wikipedia.org/wiki/List_of_European_countries_by_life_expectancy').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "WIrhopZ1HLzL"
   },
   "source": [
    "Once we have our soup, we need to extract the table containing each country's life expectancy. You can look at the page source in a browser to determine whether you can specify a class for it. In the case of our table, it did have a class of \"sortable wikitable\" so we will use that as well as the index [0] to get just the single table we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "Zn9sd5qcHJyy"
   },
   "outputs": [],
   "source": [
    "table = soup.find_all('table', attrs={'class':'sortable wikitable'})[0]\n",
    "#table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Even though we simply have one element in our 'table' object, it is important to see why we still need to specify that we only want the first element. In order to see why, let us run the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(type(soup.find_all('table',attrs={'class':'sortable wikitable'})))\n",
    "print(type(soup.find_all('table',attrs={'class':'sortable wikitable'})[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As you can see, the find_all method initially returns a so called result set -- a set in which the HTML elements it has found are being stored. In turn, the elements in this set are the elements we can actually use and call methods on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "oHg3VQ0_HQqB"
   },
   "source": [
    "We now have the table we want, but to be able to load the data into Pandas, we need to extract each of the rows (\n",
    "\n",
    "tags) and their cell values into a a nested list. We can do that with just a couple lines of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "UR43pbyjHSYF"
   },
   "outputs": [],
   "source": [
    "# Find all rows with a <tr>...</tr> (table row) element \n",
    "rows = table.find_all('tr')\n",
    "rows[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Print row[7] with text method\n",
    "rows[7].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Strip this row\n",
    "rows[7].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Split according to newline\n",
    "rows[7].text.strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Strip and split all rows\n",
    "rows = [row.text.strip().split(\"\\n\") for row in rows]\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "NLBD2Y5FHUNo"
   },
   "source": [
    "From this nested list, we can specify what the column names are and then use the rest of the data to populate a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "affR6Wu8HWY_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data = rows[1:]\n",
    "df = pd.DataFrame(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Assign values to other column\n",
    "df.iloc[0,3] = df.iloc[0,4]\n",
    "df.iloc[1,3] = df.iloc[1,4]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Drop additional columns\n",
    "df = df.drop([1,4], axis = 1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Assign first row to df as col_names\n",
    "del rows[0][1]\n",
    "df.columns = rows[0]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Rename column \n",
    "df = df.rename(columns = {'Life expectancy[1]': 'Life expectancy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df.loc[9,'Country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "S-wmDd7UHWvr"
   },
   "source": [
    "## Web Scraping Challenges\n",
    "\n",
    "The two scraping tasks we performed in this lesson were possible because the web pages were created with HTML. It is important to note that this is not always the case and that it will make your scraping efforts more difficult (if not impossible) when it is not.\n",
    "\n",
    "Aside from this, there are several other factors that may present challenges when performing web scraping. Below is a list of challenges and considerations that should be helpful to keep in mind while performing web scraping.\n",
    "\n",
    "Need to determine what information you want to extract from each page.\n",
    "Consider creating a customized scraper for each site to account for different formatting from one site to the next.\n",
    "Consider that different pages within the same site may have different structure.\n",
    "Consider that a page's content and structure can change over time.\n",
    "Terms of service for a website may not allow for scraping of their pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "57sa7Tl0Hd7G"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, we covered the basics of web scraping. We began by looking at an example of how we can scrape text from a web page using Python's requests and BeautifulSoup libraries. We then studied a more complex example where we had to extract a specific table from the HTML of a web page and then extract the rows of that table so that we could load them into a Pandas data frame. We finished up the chapter by noting some important challenges and considerations you should keep in mind while scraping. Now that you have completed this lesson, you should have the skills you need to obtain data from web pages and structure it in a way where it can be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "UyjFjXhcHcLg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Web_Scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
