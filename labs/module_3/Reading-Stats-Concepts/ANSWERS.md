
### Challenge 1: What is the difference between expected value and mean?

The expected value, is the mathematic result when evaluating the probabilities. The mean is the average result of the event actually happening. They can be considered the same for random variables with the same probabilities. However, if the variables’ probabilities differ, then the expected value would be the weighted mean for the random variables.

### Challenge 2: What is the "problem" in science with p-values?

There’s a tendency to conclude that there is ‘no difference’ or ‘no association’ just because p-value is larger than a threshold, however, this is a very superficial way to look at the results of a test. The p-value is a good measure, but should not be taken as the only way to prove or disregard a study, more in-depth analysis of the results is needed. P-values are used in a conventional and dichotomous way which is not always ideal in deciding whether a result confirms or disregards a scientific hypothesis. There’s a belief that once the threshold of statistical significance is passed, the result can be considered as valid.

### Challenge 3: Applying testing to a specific case: A/B testing.

Example used: BASECAMP (http://millions.social/tested7)
In this example a company made many changes to their website, most importantly to their landing page. In the landing page they used to include a signup form, which they removed in their newer version. Slowly but consistently the amounts of sign-ups to their website decreased and it took them 6 months to find out the problem and correct it. A simple A/B testing where users are shown either the old or the new landing page and ask them if they would sign-up, would've avoided this issue, as they would've been able to spot much sooner that removing the sign-up sheet was not a good idea. Instead they lost 6 months of potential users until they identified and corrected the issue. We can use the example of BASECAMP as a lesson on how important is to spend time and effort testing products before taking final decisions. Otherwise, you run the risk of being negatively affected by changes and possibly not being able to redeem them in the long term. 

Taking into account the example of BASECAMP, it would be beneficial for the "mini-games" application to indeed follow an A/B Testing. Within a couple of weeks, they will be able to identify whether there's a significant difference between implementing a new botton or not. By doing this, they can make a well-founded and less-risky decision on what would features work better for them. 

